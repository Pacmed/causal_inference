{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Load data\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/adam/adam/causal_inference')\n",
    "\n",
    "from causal_inference.model.cfr import UseCase\n",
    "from causal_inference.model.metrics import MMDLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# set the path\n",
    "\n",
    "os.chdir('/home/adam/adam/data/19012021/')\n",
    "\n",
    "# load the dataset\n",
    "dataset = UseCase('data_guerin_rct.csv',\n",
    "                  'pf_ratio_2h_8h_outcome',\n",
    "                  'treated',\n",
    "                  seed=1234)\n",
    "\n",
    "# calculate split\n",
    "train, test = dataset.get_splits()\n",
    "\n",
    "# prepare data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_of_features = 27\n",
    "hidden_layer_1 = 5\n",
    "hidden_layer_2 = 5\n",
    "representation_layer = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RepresentationNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RepresentationNetwork, self).__init__()\n",
    "\n",
    "        self.rep1 = nn.Linear(26, 10 * 26)\n",
    "        self.rep2 = nn.Linear(10 * 26, 10 * 26)\n",
    "        self.rep3 = nn.Linear(10 * 26, 26)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = x[:, 0]\n",
    "        t = torch.reshape(t, (t.shape[0], 1))\n",
    "        x = x[:, 1:]\n",
    "        x = nn.functional.elu(self.rep1(x))\n",
    "        x = nn.functional.elu(self.rep2(x))\n",
    "        x = self.rep3(x)\n",
    "        return torch.cat((t, x), dim=1)\n",
    "\n",
    "representation_model = RepresentationNetwork()\n",
    "\n",
    "class FactualModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FactualModel, self).__init__()\n",
    "\n",
    "        self.factual1 = nn.Linear(26, 10 * 26)\n",
    "        self.factual2 = nn.Linear(10 * 26, 10 * 26)\n",
    "        self.factual3 = nn.Linear(10 * 26, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.elu(self.factual1(x))\n",
    "        x = nn.functional.elu(self.factual2(x))\n",
    "        x = self.factual3(x)\n",
    "        return x\n",
    "\n",
    "control_model = FactualModel()\n",
    "treated_model = FactualModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Define a more flexible model\n"
    }
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(27, 4 * 27)\n",
    "        self.l2 = nn.Linear(4 * 27, 4 * 27)\n",
    "        self.do1 = nn.Dropout(0.1)\n",
    "        self.l3 = nn.Linear(4 * 27, 4 * 27)\n",
    "        self.l4 = nn.Linear(4 * 27, 4 * 27)\n",
    "        self.do2 = nn.Dropout(0.1)\n",
    "        self.l5 = nn.Linear(4 * 27, 1)\n",
    "    def forward(self, x):\n",
    "        h1 = nn.functional.relu(self.l1(x))\n",
    "        h2 = nn.functional.relu(self.l2(h1))\n",
    "        do1 = self.do1(h1 + h2)\n",
    "        h3 = nn.functional.relu(self.l3(do1))\n",
    "        h4 = nn.functional.relu(self.l2(h3))\n",
    "        do2 = self.do2(h3 + h4)\n",
    "        prediction = self.l3(do2)\n",
    "        return prediction\n",
    "\n",
    "model = ResNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Define optimizer\n"
    }
   },
   "outputs": [],
   "source": [
    "representation_params = representation_model.parameters()\n",
    "control_params = control_model.parameters()\n",
    "treated_params = treated_model.parameters()\n",
    "\n",
    "lr = 0.005\n",
    "\n",
    "optimizer = optim.Adam(params, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Define loss\n"
    }
   },
   "outputs": [],
   "source": [
    "#loss = nn.MSELoss(reduce='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Training loop\n"
    }
   },
   "outputs": [],
   "source": [
    "nb_epochs = 20\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    losses = list()\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        x, y = batch\n",
    "        x, y = x.float(), y.float()\n",
    "\n",
    "        # temporary\n",
    "        x[x != x] = 0\n",
    "\n",
    "        # x has always t in the first column\n",
    "\n",
    "        # 1. forward\n",
    "        representation = model(x.float())\n",
    "\n",
    "        # 2. objective function\n",
    "        objective = loss(representation, y.float())\n",
    "\n",
    "        # 3. cleaning the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # 4. compute gradients\n",
    "        objective.backward()\n",
    "\n",
    "        # 5. update weights\n",
    "        optimizer.step()\n",
    "        # manual grad update\n",
    "        # with torch.no_grad(): params = params - eta * params.grad\n",
    "\n",
    "        losses.append(objective.item())\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, train loss: {torch.tensor(losses).mean():.2f}')\n",
    "\n",
    "    losses = list()\n",
    "    model.eval()\n",
    "    for batch in test_loader:\n",
    "        x, y = batch\n",
    "        x, y = x.float(), y.float()\n",
    "\n",
    "        # x has always t in the first column\n",
    "\n",
    "        # temporary\n",
    "        x[x != x] = 0\n",
    "\n",
    "        # 1. forward\n",
    "        with torch.no_grad():\n",
    "            representation = model(x)\n",
    "\n",
    "        # 2. objective function\n",
    "        objective = loss(representation, y)\n",
    "\n",
    "\n",
    "\n",
    "        losses.append(objective.item())\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, valid loss: {torch.tensor(losses).mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from causal_inference.model.metrics import mmd_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "representation_loss = MMDLoss(kernel='multiscale')\n",
    "factual_loss = nn.MSELoss(reduce='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Representation\n"
    }
   },
   "outputs": [],
   "source": [
    "nb_epochs = 1\n",
    "for epoch in range(nb_epochs):\n",
    "    losses = list()\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        # x has always t in the first column\n",
    "        x, y = batch\n",
    "        x, y = x.float(), y.float()\n",
    "\n",
    "        # temporary\n",
    "        x[x != x] = 0\n",
    "\n",
    "        # 1. Representation Training\n",
    "        representation = representation_model(x.float())\n",
    "        representation_objective = representation_loss(representation, y)\n",
    "        representation_model.zero_grad()\n",
    "        representation_objective.backward()\n",
    "\n",
    "        x_control = x[x[:, 0] == 0, 1:]\n",
    "        y_control = y[x[:, 0] == 0]\n",
    "        x_treated = x[x[:, 0] == 1, 1:]\n",
    "        y_treated = y[x[:, 0] == 1]\n",
    "\n",
    "        nb_control = x_control.shape[0]\n",
    "        nb_treated = x_treated.shape[0]\n",
    "\n",
    "        if nb_control > 0:\n",
    "            output = control_model(x_control.float())\n",
    "            control_objective = factual_loss(output, y_control)\n",
    "            control_model.zero_grad()\n",
    "            control_objective.backward()\n",
    "\n",
    "        if nb_treated > 0:\n",
    "            output = treated_model(x_treated.float())\n",
    "            treated_objective = factual_loss(output, y_treated)\n",
    "            treated_model.zero_grad()\n",
    "            treated_objective.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #print(representation_model.parameters())\n",
    "            #print(representation_objective.grad)\n",
    "            for p in model.parameters():\n",
    "                print(p.data)\n",
    "                print(lr)\n",
    "                print(representation_objective.grad.data)\n",
    "                #p.data -= learning_rate * param.grad.data\n",
    "             #   new_val = update_function(p, p.grad, loss, other_params)\n",
    "              #  p.copy_(new_val)\n",
    "\n",
    "        #param - learning_rate * grad\n",
    "        # manual grad update\n",
    "        # with torch.no_grad(): params = params - eta * params.grad\n",
    "\n",
    "        losses.append(objective.item())\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, train loss: {torch.tensor(losses).mean():.2f}')\n",
    "\n",
    "    losses = list()\n",
    "    model.eval()\n",
    "    for batch in test_loader:\n",
    "        x, y = batch\n",
    "        x, y = x.float(), y.float()\n",
    "\n",
    "        # x has always t in the first column\n",
    "\n",
    "        # temporary\n",
    "        x[x != x] = 0\n",
    "\n",
    "        # 1. forward\n",
    "        with torch.no_grad():\n",
    "            representation = model(x)\n",
    "\n",
    "        # 2. objective function\n",
    "        objective = loss(representation, y)\n",
    "\n",
    "        losses.append(objective.item())\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, valid loss: {torch.tensor(losses).mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "        param.data -= learning_rate * param.grad.data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
