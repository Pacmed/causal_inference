{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from causalinference import CausalModel\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Load the data\n"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/adam/files/data/04012020/')\n",
    "df = pd.read_csv('data_guerin_rct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% drop outcome\n"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['pf_ratio_4h_outcome'], inplace=True)\n",
    "df.dropna(subset=['pf_ratio_12h_outcome'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.info(max_cols=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "COLS = ['lactate',\n",
    "        'tidal_volume',\n",
    "        'respiratory_rate_measured',\n",
    "        'peep',\n",
    "        'fio2']\n",
    "        #'lung_compliance_static'\n",
    "\n",
    "# tidal volume is highly correlated with lung_compliance\n",
    "# and tidal volume is more imbalanced and have more observations\n",
    "\n",
    "COLS_bool = df.filter(regex='med').columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#df_one_hot_encoded = pd.get_dummies(df[COLS_bool])\n",
    "#df_one_hot_encoded.drop(columns=df_one_hot_encoded.\n",
    "#                        filter(regex='False').\n",
    "#                        filter(regex=\"nice\").\n",
    "#                        columns.\n",
    "#                        to_list(),\n",
    "#                        inplace=True)\n",
    "#\n",
    "#df_one_hot_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df[COLS].corr().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "treated = df.iloc[:,0].values.astype('int')\n",
    "t = df.loc[:, 'treated'].values\n",
    "\n",
    "X_num = df[COLS].values\n",
    "X_bool = df[COLS_bool].values\n",
    "\n",
    "y = df.loc[:, 'pf_ratio_12h_outcome'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Impute missing values\n"
    }
   },
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(X_num)\n",
    "X_num = imp.transform(X_num)\n",
    "\n",
    "# Standardize the predictors\n",
    "scaler = StandardScaler().fit(X_num)\n",
    "X_num = scaler.transform(X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(X_num.shape)\n",
    "print(X_bool.shape)\n",
    "\n",
    "X = np.hstack((X_num, X_bool))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Causal modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate CausalModel\n",
    "\n",
    "causal = CausalModel(y, t, X)\n",
    "print(causal.summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "COLS = COLS_num + COLS_bool\n",
    "X_names = df[COLS].drop(columns=['treated', 'pf_ratio_12h_outcome']).columns.to_list()\n",
    "ndiff = causal.summary_stats['ndiff']\n",
    "new_dict = {k: round(v, 2) for k, v in zip(X_names, ndiff)}\n",
    "print(new_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_summary = pd.DataFrame({'ndiff': ndiff}, index=X_names)\n",
    "df_summary['ndiff'] = df_summary['ndiff'].map(lambda x: round(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "SEED = 1234\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=SEED,\n",
    "                         class_weight='balanced',\n",
    "                         penalty='none').fit(X, t)\n",
    "\n",
    "pscore = clf.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(pscore[t],\n",
    "             hist = True,\n",
    "             kde = True,\n",
    "             label='Prone')\n",
    "\n",
    "sns.distplot(pscore[~t],\n",
    "             hist = True,\n",
    "             kde = True,\n",
    "             label='Supine')\n",
    "\n",
    "# Plot formatting\n",
    "plt.legend(prop={'size': 12})\n",
    "plt.title('Pscore')\n",
    "plt.xlabel('pscore')\n",
    "plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "causal.raw_data._dict['pscore'] = pscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We assign the new p-score\n",
    "causal.raw_data._dict['pscore'] = pscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trim samples to ensure positivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "causal.trim_s()\n",
    "print(causal.cutoff)\n",
    "print(causal.summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "causal.stratify_s()\n",
    "print(causal.strata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for stratum in causal.strata:\n",
    "    print(max(stratum.summary_stats['ndiff']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that the imbalance decreased a little, but there is\n",
    "still lot to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#causal.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "causal.est_via_ols()\n",
    "print(causal.estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for stratum in causal.strata:\n",
    "    stratum.est_via_blocking()\n",
    "[stratum.estimates['blocking']['ate'] for stratum in causal.strata]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the sample-weighted average of the above within-bin least squares estimates results in a propensity score\n",
    "matching estimator that is commonly known as the blocking estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# sample-weighted average of the within-bin least squares estimates\n",
    "\n",
    "causal.est_via_blocking()\n",
    "print(causal.estimates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "causal.est_via_matching(bias_adj=True)\n",
    "print(causal.estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for stratum in causal.strata:\n",
    "    stratum.est_via_matching()\n",
    "[stratum.estimates['matching']['ate'] for stratum in causal.strata]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "causal.est_via_weighting()\n",
    "print(causal.estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y = []\n",
    "yerr = []\n",
    "x_label = []\n",
    "\n",
    "for method, result in dict(causal.estimates).items():\n",
    "    y.append(result[\"ate\"])\n",
    "    yerr.append(result[\"ate_se\"])\n",
    "    x_label.append(method)\n",
    "\n",
    "y.append(3)\n",
    "yerr.append(0)\n",
    "x_label.append(\"raw\")\n",
    "\n",
    "x = np.arange(len(y))\n",
    "\n",
    "plt.errorbar(x=x, y=y, yerr=yerr, linestyle=\"none\", capsize=5, marker=\"o\")\n",
    "plt.xticks(x, x_label)\n",
    "plt.title(\"Estimated Effect Size\", fontsize=18)\n",
    "plt.hlines(y=13, xmin=-0.5, xmax = 4.5, linestyles=\"dashed\")\n",
    "#plt.xlim(-0.5,3.5);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
