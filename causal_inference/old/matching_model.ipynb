{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Estimating treatment effects with matching\n",
    "\n",
    "\n",
    "The goal of this notebook is to estimate causal treatment effects using observational i.e. nonrandomized data.\n",
    "\n",
    "One of the methods to achieve this goal is to use matching. The aim of any matching method is to reduce the bias of\n",
    "an observational data set. This mean reducing the dissimilarity between the covariates distribution in the\n",
    "treated $p(x \\mid t = 1)$ and control $p(x \\mid t = 0)$ groups. In theory, under the strong ignorability assumption,\n",
    "the matched data set will mimic an RCT. Hence we can treat $X$ as independent of $T$ resulting\n",
    "in $p^{t=1}(x) \\approx p^{t=0}(x)$ [last sentence to be checked and citation needed].\n",
    "\n",
    "The most popular matching based method is the propensity score matching [1]. It uses an estimatand of the true\n",
    "propensity score to match the data. Hence observations with closest propensity scores will be matched.\n",
    "\n",
    "There is a typical workflow that a causal study should follow. We will present it in this notebook based on the\n",
    "package:\n",
    "https://github.com/laurencium/causalinference\n",
    "and with summary of workflow:\n",
    "https://laurencewong.com/software/conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading the data\n",
    "\n",
    "We load synthetic data using uberml package for making causal inference. We can choose from five different scenarios\n",
    "to generate the data:\n",
    "1. difficult nuisance components and an easy treatment effect;\n",
    "2. a randomized trial; 3 an easy propensity and a difficult baseline;\n",
    "4. unrelated treatment and control groups;\n",
    "5. a hidden confounder biasing treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from causalml.dataset import synthetic_data\n",
    "\n",
    "y, X, treatment, true_ite, expected_outcome, true_propensity = synthetic_data(mode=1, n=1000, p=10, sigma=1.0)\n",
    "\n",
    "# As the mean propensity doesn't say lot it would be nice to plot the true propensity to see the overlap between\n",
    "# control and treated.\n",
    "# It should be done like: http://ethen8181.github.io/machine-learning/ab_tests/causal_inference/matching.html\n",
    "\n",
    "\n",
    "def calculate_ate(ite):\n",
    "    return ite.mean().round(2), ite.std().round(2)\n",
    "\n",
    "def calculate_propensity(propensity):\n",
    "    return propensity.mean().round(2), propensity.std().round(2)\n",
    "\n",
    "print(\"The true ATE of the generated data is\",\n",
    "      calculate_ate(true_ite)[0],\n",
    "      \"with standard deviation equal to\",\n",
    "      calculate_ate(true_ite)[1],\n",
    "      \".\")\n",
    "\n",
    "print(\"The average propensity score value is equal to\",\n",
    "      calculate_propensity(true_propensity)[0],\n",
    "      \"with standard deviation equal to\",\n",
    "      calculate_propensity(true_propensity)[1],\n",
    "      \".\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Design Phase\n",
    "\n",
    "We begin by a design phase. Let's instatiate the causal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from causalinference import CausalModel\n",
    "\n",
    "causal = CausalModel(y, treatment, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### a. Accessing the covariate balance\n",
    "\n",
    "We begin with accessing the initial covariate balance. This allows us to choose the right method and also\n",
    "access how successful we were in de-biasing the data.\n",
    "\n",
    "There is no agreement on how to access balance. We can investigate the difference in momements of each covariate.\n",
    "A popular metric proposed by [citation needed] is the normalized difference in a covariate averages:\n",
    "\n",
    "$$\\frac{\\overline{X}_i^{t=1} - \\overline{X}_i^{t=0}}{\\sqrt{\\frac{1}{2}(s_i^{t=1})^2 + (s_i^{t=0})^2}}$$\n",
    "\n",
    "This quantity can be easily access using the package and is showed under the Nor-diff column\n",
    "\n",
    "To do: it would be interesting to see if it is related to the IPM term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(causal.summary_stats)\n",
    "\n",
    "print(\"The maximum distance is\", np.abs(causal.summary_stats['ndiff']).max().round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To match on propensity we need to estimate the propensity score.\n",
    "\n",
    "### 2. Estimating the propensity score\n",
    "\n",
    "With the package we have two options of estimating the propensity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Estimate propensity to improve the balance\n",
    "\n",
    "causal.est_propensity()\n",
    "print(causal.propensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Use an algorithm for variable selection from Imbens to improve the balance\n",
    "# Details in https://laurencewong.com/software/propensity-score\n",
    "causal.est_propensity_s()\n",
    "print(causal.propensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# To do in the analysis: supply your own propensity model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We may want to exclude cases that almost surely receive the treatment or not in order to analyze only more similiar\n",
    "# observations. The logic behind this step is that regions with high propensity corresponds to the regions\n",
    "# with a lack of overlap.\n",
    "\n",
    "causal.trim_s()\n",
    "causal.cutoff\n",
    "print(causal.summary_stats)\n",
    "\n",
    "#As you can see the number of observation have changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4. Stratification\n",
    "\n",
    "An easy method to access whether the propensity score is helpful is to perform a stratification on the data set.\n",
    "If the bins have less bias than the whole data then we are doing a good job. We again can choose from two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Now to access if the propensity is helpful we should see how it balances the strata\n",
    "causal.blocks = 5\n",
    "causal.stratify()\n",
    "print(causal.strata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Using a data drive algorithm outlined in https://laurencewong.com/software/stratification\n",
    "causal.reset()\n",
    "causal.est_propensity_s()\n",
    "causal.trim_s()\n",
    "causal.stratify_s()\n",
    "print(causal.strata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We can print the maximum imbalance for each bin\n",
    "\n",
    "for stratum in causal.strata:\n",
    "    print(np.absolute(stratum.summary_stats['ndiff']).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysis phase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "What we could do now is to fit an OLS models to each of the stratified sub-samples and weight the resulted model to\n",
    "obtain the first estimate of ATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "causal.est_via_blocking()\n",
    "print(causal.estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "However we can do better by using a matching estimator. This package matches however not on the propensity\n",
    "score, but tries to find the best match in the covariate space by using nearest neighborhood matching. After performing the matching the algorithm takes\n",
    "the average of the outcomes in both groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#invoke matching estimator\n",
    "\n",
    "causal.est_via_matching()\n",
    "print(causal.estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can improve the above estimate by adjusting for bias.\n",
    "\n",
    "Let $m$ be the matching function. In general $X_i$ and $X_{m(i)}$ will not be similiar so the matching estimator will\n",
    "be additionally biased.\n",
    "\n",
    "The package is not really explaining what does adjusting on bias means. Under the hood it is modifying each ITE\n",
    "by approximated by the dot product of the matching discrepancy (i.e., X-X_matched) and the\n",
    "coefficients from the bias correction regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "causal.est_via_matching(bias_adj=True)\n",
    "print(causal.estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Conclusions\n",
    "\n",
    "This approach i.e. vanilla k-NN matching on covariates it's certainly not satisfactory. The next step is to look\n",
    "how we can perform propensity score matching. However with propensity scores there is a problem reported\n",
    "in the literature, namely it can increase the balance easily:\n",
    "https://gking.harvard.edu/files/gking/files/psnot.pdf\n",
    "\n",
    "Further steps would include matching on the logit of the propensity score and look at what is done in case\n",
    "studies of performing matching:\n",
    "\n",
    "https://www.tandfonline.com/doi/pdf/10.1080/00273171.2011.540480\n",
    "\n",
    "https://sci-hub.do/http://jhr.uwpress.org/content/50/2/373.full.pdf+html\n",
    "\n",
    "\n",
    "There are also notebooks available to check for some ideas\n",
    "http://www.degeneratestate.org/posts/2018/Mar/24/causal-inference-with-python-part-1-potential-outcomes/\n",
    "\n",
    "Talks to watch:\n",
    "https://www.youtube.com/watch?v=rBv39pK1iEs\n",
    "https://www.youtube.com/watch?v=gaUgW7NWai8\n",
    "\n",
    "## 4. Further considerations\n",
    "When implementing an estimator in the future consider using this\n",
    "https://scikit-learn.org/stable/developers/develop.html\n",
    "https://sklearn-template.readthedocs.io/en/latest/user_guide.html\n",
    "\n",
    "There are clever algorithm like Generative Matching that can exploit a loss function of our choice\n",
    "and we could choose just the IPM or KL or whatever and this is worth analyzing. This is however written in R.\n",
    "\n",
    "## 5. Uber's causal inference package\n",
    "\n",
    "https://github.com/uber/causalml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from causalml.dataset import synthetic_data\n",
    "from causalml.propensity import ElasticNetPropensityModel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y, X, treatment, _, _, e = synthetic_data(mode=1, n=1000, p=5, sigma=1.0)\n",
    "\n",
    "df = pd.DataFrame(X, columns=['x'+ str(i) for i in range(X.shape[1])])\n",
    "df['treatment'] = treatment\n",
    "df['outcome'] = y\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pm = ElasticNetPropensityModel()\n",
    "ps = pm.fit_predict(X, treatment)\n",
    "\n",
    "df['propensity'] = ps\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from causalml.match import NearestNeighborMatch, create_table_one\n",
    "\n",
    "\"\"\"\n",
    "    Propensity score matching based on the nearest neighbor algorithm.\n",
    "    Attributes:\n",
    "        caliper (float): threshold to be considered as a match.\n",
    "        replace (bool): whether to match with replacement or not\n",
    "        ratio (int): ratio of control / treatment to be matched. used only if\n",
    "            replace=True.\n",
    "        shuffle (bool): whether to shuffle the treatment group data before\n",
    "            matching\n",
    "        random_state (numpy.random.RandomState or int): RandomState or an int\n",
    "            seed\n",
    "\"\"\"\n",
    "\n",
    "psm = NearestNeighborMatch(replace=False,\n",
    "                           ratio=1,\n",
    "                           random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "matched = psm.match(data=df,\n",
    "                    treatment_col= 'treatment',\n",
    "                    score_cols= ['propensity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "matched.treatment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "create_table_one(data=matched,\n",
    "                 treatment_col= 'treatment',\n",
    "                 features=matched.columns.tolist()[0:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Now we can estimate the treatment effect simply by taking the average\n",
    "\n",
    "def ate_matched(df, treatment_col = 'treatment', outcome_col = 'outcome'):\n",
    "    df_control = df[df[treatment_col] == 0]\n",
    "    df_treated = df[df[treatment_col] == 1]\n",
    "    ate = df_treated[outcome_col].mean() - df_control[outcome_col].mean()\n",
    "    return ate.round(2)\n",
    "\n",
    "print(\"The average treatment effect estimated by propensity score matching is equal to\",\n",
    "      ate_matched(matched))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To do:\n",
    "1. Look how it performs on multiple iterations with randomized data generating function. For this first make data modular.\n",
    "2. Maybe we could use the cross validation metrics to tune the parameters\n",
    "3. A sensible approach would be to stratify the sample into small bins based on some specified loss and then use\n",
    "a blocking estimator or even estimate the pscore to improve the balance.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
