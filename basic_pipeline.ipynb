{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from causalml.dataset import synthetic_data\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import causal_inference.propensity as causal\n",
    "\n",
    "reload(causal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_ate(ite):\n",
    "    return ite.mean().round(2), ite.std().round(2)\n",
    "\n",
    "def calculate_propensity(propensity):\n",
    "    return propensity.mean().round(2), propensity.std().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "y, X, treatment, true_ite, expected_outcome, true_propensity = synthetic_data(mode=1,\n",
    "                                                                              n=1000,\n",
    "                                                                              p=10,\n",
    "                                                                              sigma=2)\n",
    "\n",
    "# As the mean propensity doesn't say lot it would be nice to plot the true propensity to see the overlap between\n",
    "# control and treated.\n",
    "# It should be done like: http://ethen8181.github.io/machine-learning/ab_tests/causal_inference/matching.html\n",
    "\n",
    "print(\"The true ATE of the generated data is\",\n",
    "      calculate_ate(true_ite)[0],\n",
    "      \"with standard deviation equal to\",\n",
    "      calculate_ate(true_ite)[1],\n",
    "      \".\")\n",
    "\n",
    "print(\"The average propensity score value is equal to\",\n",
    "      calculate_propensity(true_propensity)[0],\n",
    "      \"with standard deviation equal to\",\n",
    "      calculate_propensity(true_propensity)[1],\n",
    "      \".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Make a df from it\n",
    "\n",
    "df = pd.DataFrame(X, columns = ['x' + str(i) for i in range(X.shape[1])])\n",
    "df['treatment'] = treatment\n",
    "df['outcome'] = y\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "sns.kdeplot(df[df.treatment == 0]['outcome'], label=\"untreated\")\n",
    "sns.kdeplot(df[df.treatment == 1]['outcome'], label=\"untreated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The true ATE of the generated data is equal to\",\n",
    "      calculate_ate(true_ite)[0],\".\")\n",
    "print(\"Estimated ATE is equal to\",\n",
    "      calculate_ate(df[df.treatment == 1]['outcome'])[0] - calculate_ate(df[df.treatment == 0]['outcome'])[0],\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "from scipy import stats\n",
    "\n",
    "def calc_ndiff(covariate_control, covariate_treated):\n",
    "    m_c = covariate_control.mean()\n",
    "    m_t = covariate_treated.mean()\n",
    "    std_c = covariate_control.std()\n",
    "    std_t = covariate_treated.std()\n",
    "    ndiff = (m_t-m_c) / np.sqrt((std_c**2+std_t**2)/2)\n",
    "    return ndiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_summary = pd.DataFrame(index = df.iloc[:,0:X.shape[1]].columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(X.shape[1]):\n",
    "    covariate_control = df[df.treatment == 0].iloc[:,i]\n",
    "    covariate_treated = df[df.treatment == 1].iloc[:,i]\n",
    "    df_summary.iloc[i,:]['norm-diff'] = calc_ndiff(covariate_control, covariate_treated)\n",
    "    print(wasserstein_distance(covariate_control,covariate_treated))\n",
    "    # If p-value is low then we can reject the null hypothesis that\n",
    "    # the distributions of the two samples are the same\n",
    "    print(stats.ks_2samp(covariate_control,covariate_treated)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Similiar way to do the below, probably better. Check this. Goal Gio: prepare Table 1 on COVID data.\n",
    "\n",
    "# http://ethen8181.github.io/machine-learning/ab_tests/causal_inference/matching.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "norm_diff, w_dist, ks_test = [], [], []\n",
    "\n",
    "# make it an iteration over the methods\n",
    "# maybe it should be a more pythonic way to do it but less clear?\n",
    "\n",
    "for index, row in df_summary.iterrows():\n",
    "    idx = int(index[1])\n",
    "    covariate_control = df[df.treatment == 0].iloc[:,idx]\n",
    "    covariate_treated = df[df.treatment == 1].iloc[:,idx]\n",
    "    norm_diff.append(calc_ndiff(covariate_control, covariate_treated).round(2))\n",
    "    w_dist.append(wasserstein_distance(covariate_control,covariate_treated).round(2))\n",
    "    ks_test.append(stats.ks_2samp(covariate_control,covariate_treated)[1].round(3))\n",
    "\n",
    "df_summary['norm_dist'] = norm_diff\n",
    "df_summary['w_dist'] = w_dist\n",
    "df_summary['ks_test'] = ks_test\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# can we the same tests on a weighted sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#First analyze the propensity\n",
    "\n",
    "df['p_score'] = true_propensity\n",
    "p_score_control = df[df.treatment == 0]['p_score']\n",
    "p_score_treated = df[df.treatment == 1]['p_score']\n",
    "print('treatment count:', p_score_control.shape)\n",
    "print('control count:', p_score_treated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Kind of implements matching\n",
    "\n",
    "# http://ethen8181.github.io/machine-learning/ab_tests/causal_inference/matching.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# statistical tests http://benalexkeen.com/comparative-statistics-in-python-using-scipy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 8, 6\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "sns.distplot(p_score_control, label='control')\n",
    "sns.distplot(p_score_treated, label='treated')\n",
    "plt.xlim(0, 1)\n",
    "plt.title('Propensity Score Distribution of Control vs Treatment')\n",
    "plt.ylabel('Density')\n",
    "plt.xlabel('Scores')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_new = np.hstack((X, treatment.reshape(len(treatment), 1)))\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_new, y,\n",
    "                                                                train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "model = LinearRegression()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_pipeline = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('estimate_propensity', causal.PropensityEstimator()),\n",
    "    ('model', model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Preprocessing of training data, fit model\n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_squared_error(y_valid, preds)\n",
    "print('MSE:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# define the model cross-validation configuration\n",
    "\n",
    "cv = KFold(n_splits=10)\n",
    "\n",
    "# evaluate the pipeline using cross validation and calculate MAE\n",
    "scores = cross_val_score(my_pipeline,\n",
    "                         X_train, y_train,\n",
    "                         scoring='neg_mean_squared_error',\n",
    "                         cv=cv, n_jobs=-1)\n",
    "\n",
    "# convert MAE scores to positive values\n",
    "scores\n",
    "# summarize the model performance\n",
    "print('MSE: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "scores = cross_val_score(my_pipeline, X_new, y, cv=5,\n",
    "                         scoring = 'neg_mean_squared_error')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "scoring = ['neg_mean_squared_error', 'r2']\n",
    "\n",
    "scores = cross_validate(model, X_new, y, scoring=scoring)\n",
    "\n",
    "sorted(scores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scores['fit_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "scoring = {'prec_macro': 'precision_macro',\n",
    "           'rec_macro': make_scorer(recall_score, average='macro')}\n",
    "\n",
    "scores = cross_validate(model, X, y, scoring=scoring,\n",
    "                        cv=5, return_train_score=True)\n",
    "\n",
    "sorted(scores.keys())\n",
    "scores['train_rec_macro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# What is the godness of fit metric?\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "distances[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "indices[731]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "indices.reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
